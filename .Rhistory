install.packages("devtools")
getwd()
install.packages("kernsmooth")
install.packages("KernSmooth")
library(KernSmooth)
pwd
getwd()
getwd()
library(KernSmooth)
getwd()
getwd()
setwd("D:\Coursera")
setwd("D:\\Coursera")
getwd()
pollutantmean <- function(directory, pollutant, id = 1:332) {
file_list <- list.files(directory)
all_data <- lapply(file.path(directory,file_list),read.csv)
all_data = do.call(rbind.data.frame,all_data)
mean(Data[,pollutant],na.rm=TRUE)
}
pollutantmean("./specdata","sulfate")
pollutantmean <- function(directory, pollutant, id = 1:332) {
file_list <- list.files(directory)
all_data <- lapply(file.path(directory,file_list),read.csv)
all_data = do.call(rbind.data.frame,all_data)
mean(all_data[,pollutant],na.rm=TRUE)
}
pollutantmean("./specdata","sulfate")
pollutantmean("specdata","sulfate")
pollutantmean <- function(directory, pollutant, id = 1:332) {
file_list <- list.files(directory)
fileNames <- as.numeric(sub("\\.csv$","",file_list))
sel_files <- file_list[match(id,fileNames)]
all_data <- lapply(file.path(directory,sel_files),read.csv)
all_data <- do.call(rbind.data.frame,all_data)
mean(Data[,pollutant],na.rm=TRUE)
}
pollutantmean("specdata","sulfate",1:10)
pollutantmean <- function(directory, pollutant, id = 1:332) {
file_list <- list.files(directory)
fileNames <- as.numeric(sub("\\.csv$","",file_list))
sel_files <- file_list[match(id,fileNames)]
all_data <- lapply(file.path(directory,sel_files),read.csv)
all_data <- do.call(rbind.data.frame,all_data)
mean(all_data[,pollutant],na.rm=TRUE)
}
pollutantmean("specdata","sulfate",1:10)
pollutantmean("specdata", "nitrate", 70:72)
pollutantmean("specdata", "nitrate", 23)
complete <- function(directory, id = 1:332) {
id_len <- length(id)
complete_data <- rep(0, id_len)
all_files <- as.character( list.files(directory) )
file_paths <- paste(directory, all_files, sep="")
j <- 1
for (i in id) {
current_file <- read.csv(file_paths[i], header=T, sep=",")
complete_data[j] <- sum(complete.cases(current_file))
j <- j + 1
}
result <- data.frame(id = id, nobs = complete_data)
return(result)
}
complete("specdata", 1)
complete <- function(directory, id = 1:332) {
id_len <- length(id)
directory <- paste(directory, "\\", sep="")
complete_data <- rep(0, id_len)
all_files <- as.character( list.files(directory) )
file_paths <- paste(directory, all_files, sep="")
j <- 1
for (i in id) {
current_file <- read.csv(file_paths[i], header=T, sep=",")
complete_data[j] <- sum(complete.cases(current_file))
j <- j + 1
}
result <- data.frame(id = id, nobs = complete_data)
return(result)
}
complete("specdata", 1)
complete("specdata", c(2, 4, 8, 10, 12))
complete("specdata", 30:25)
complete("specdata", 3)
corr <- function(directory, threshold = 0) {
all_files <- list.files( directory )
corre <- c()
for(f in 1:length(files)){
data <- read.csv( paste(directory, "/", files[f], sep="") )
data <- data[complete.cases(data),]
if ( nrow(data) > threshold ) {
corre <- c(corre, cor(data$sulfate, data$nitrate) )
}
}
return( corre )
}
source("corr.R")
source("complete.R")
cr <- corr("specdata", 150)
head(cr)
corr("specdata", 150)
corr <- function(directory, threshold = 0) {
all_files <- list.files( directory )
corre <- c()
for(f in 1:length(all_files)){
data <- read.csv( paste(directory, "/", all_files[f], sep="") )
data <- data[complete.cases(data),]
if ( nrow(data) > threshold ) {
corre <- c(corre, cor(data$sulfate, data$nitrate) )
}
}
return( corre )
}
corr("specdata", 150)
cr <- corr("specdata", 150)
head(cr)
summary(cr)
cr <- corr("specdata", 5000)
summary(cr)
cr <- corr("specdata")
summary(cr)
source("submitscript1.R")
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
install_from_swirl("Getting and Cleaning Data")
install_course_zip("C:\\Users\\poojan\\Downloads\\swirl_courses-master.zip", multi=TRUE)
swirl()
swirl()
library(swirl)
uninstall_all_courses()
install_course_zip("C:\\Users\\poojan\\Downloads\\swirl_courses-master.zip", multi=TRUE)
swirl()
mydf <- read.csv(path2csv,stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
packageVersion(dpylr)
packageVersion("dpylr")
packageVersion("dplyr")
cran <- tbl_df(mydf)
rm("mydf")
cran
bye
bye()
swirl()
getwd()
swirl()
install.packages("swirl")
library(swirl)
install_from_swirl("Getting and Cleaning Data")
swirl()
read.csv(path2csv,stringsAsFactors = FALSE)
mydf <- read.csv(path2csv,stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
packageversion(dplyr)
packageversion("dplyr")
packageVersion("dplyr")
cran <- tbl_df(mydf)
rm("mydf")
cran
?select
select(cran,ip_id,package,country)
%:20
5:20
select(crn,r_arch:country)
select(cran,r_arch:country)
select(cran,country:r_arch)
cran
select(cran, -time)
-5:20
-(5:20)
select(cran,-(x:size))
select(cran,-(X:size))
filter(cran, package == "swirl")
filter(cran, r_version == "3.1.1", country == "US")
?Comparision
?Comparison
filter(cran, r_version <= "3.0.2", country == "IN")
filter(cran, country == "US" | country =="IN")
filter(cran,size > 100500, r_os == "linux-gnu")
is.na(c(3,5,NA,10))
!is.na(c(3,5,NA,10))
filter(cran,!is.na(r_version))
cran2 <- select(cran, size:ip_id)
arrange(cran2,ip_id)
arrange(cran2,desc(ip_id)
)
arrange(cran2, package, ip_id)
arrange(cran2, country, desc(r_version),ip_id)
cran3 <- select(cran, ip_id, package, size)
cran3
mutate(cran3, size_mb = size/2^20)
mutate(cran3, size_mb = size/2^20, size_gb = size_mb/2^10)
mutate(cran3, correct_size = size + 1000)
summarize(cran, avg_bytes = mean(size))
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(url=fileUrl,destfile="idaho_housing.csv",mode="w",method="curl")
data <- read.csv("idaho_housing.csv")
data <-read.csv("getdata_data_ss06hid")
data <-read.csv("getdata_data_ss06hid.csv")
data
head(data)
install.packages("httpuv")
library(httr)
library(httpuv)
myapp <- oauth_app("github",
key = "d92963201d65c39d173b",
secret = "a774e9ab60e8f55c99b542a8d9419436640e23e3")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
gtoken <- config(token = github_token)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
library(swirl)
ls()
rm(list=ls())
ls()
install_from_swirl("Getting and Cleaning Data")
swirl()
library(dplyr)
cran <- tbl_df(mydf)
rm(mydf)
rm("mydf")
cran
?group_by
by_package <- group_by(cran,packacge)
by_package <- group_by(cran,package)
by_package
summarise(by_package,mean(size))
submit()
submit()
pack_sum
quantile(pack_sum$count,probs = 0.99)
top_counts <- filter(pack_sum, count > 679)
top_counts
view(top_counts)
View(top_counts)
top_counts_sorted <- arrange(top_counts, desc(counts))
top_counts_sorted <- arrange(top_counts, desc(count))
View(top_counts_sorted)
quantile(pack_sum$unique, probs=0.99)
top_unique <- filter(pack_sum, unique > 465)
View(top_unique)
top_unique_sorted <- arrange(top_unique, desc(unique)
)
View(top_unique_sorted)
submit()
sumbit()
submit()
submit
submit()
View(result3)
submit()
submit()
submit()
submit()
library(tidyr)
students
?gather
gather(students,sex,count,-grade)
students2
res <- gather(students2,sex_class,count,-grade)
res
?seperate
?separate
separate(res,sex_class,into=c("sex","class"))
submit()
submit()
students3
submit()
?spread
submit()
extract_numeric("class5")
submit()
students4
submit()
submit()
submit()
passed
failed
passed <- passed %>% mutate(status = "passed")
failed <- failed %>% mutate(status="failed")
bind_rows(passed,failed)
sat
submit()
submit()
submit()
ls
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv","q3_1.csv")
data <- read.csv(q3_1,header = TRUE)
data <- read.csv("q3_1.csv",header = TRUE)
View(data)
filter(data,acr>10)
filter(data,ACR>10)
data1 <- filter(data,  ACR > 10)
View(data1)
data1 <- filter(data,  ACR = 10)
data1 <- filter(data,  ACR == 3)
View(data1)
data2 <- filter(data1, ags == 6)
data2 <- filter(data1, AGS == 6)
which(data$ACR==3 & data$AGS == 6)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg","pic.jpg")
pic <- read.csv(pic.jpg)
pic <- read.csv("pic.jpg")
install.packages("jpeg")
library(jpeg)
?jpeg
pic <- jpeg("pic.jpg")
View(pic)
pic
jpeg("pic.jpg")
print(jpeg("pic.jpg"))
img <- readJPEG(system.file("img", "pic.jpg", package="jpeg"))
img <- readJPEG("pic.jpg")
ls
img <- readJPEG("https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg", native = TRUE)
img <- readJPEG(download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg"), native = TRUE)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg","pic.jpg",mode = "wb")
img <- readJPEG(download.file("pic.jpg", native = TRUE)
)
img <- readJPEG("pic.jpg", native = TRUE)
img
quantile(img, probs = 0.3,0.8)
quantile(img, probs = 0.8)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv","gdp.csv")
gdb <- tbl_df(read.csv("gdp.csv"))
View(gdp)
View(gdb)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv","edu.csv")
edu <- read.csv("edu.csv")
View(edu)
edutbl <- tbl_df(edu)
View (edutbl)
data1 <- sum(gdb$x==edu$CountryCode)
data1
cedu <- select(edu,CountryCode)
cedu
cgdp <- select (gdb,X)
cgdp
cgdp <- select (gdp,X)
gdp <- read.csv("gdp.csv")
cgdp <- select (gdp,X)
cgdp
cgdp <- filter(cgdp,!is.na(CountryCode))
cgdp <- filter(cgdp,!is.na(X))
cgdp
library(swirl)
rm(list=ls())
install_from_swirl("Getting and Cleaning Data")
install_from_swirl("Getting and Cleaning Data")
install_from_swirl("Getting and Cleaning Data")
library(swirl)
install_from_swirl("Getting and Cleaning Data")
library(swirl)
install_from_swirl("Exploratory Data Analysis")
swirl()
bye()
setwd("D:\\Coursera\\datasciencecoursera\\RepData_PeerAssessment1")
library(knitr)
install.packages("knitr")
library(knitr)
knit2html("PA1_template.Rmd")
knit2html("PA1_template.Rmd")
install.packages("rmarkdown")
library(rmarkdown)
knit2html("PA1_template.Rmd")
render_html("PA1_template.Rmd")
render("PA1_template.Rmd",output_format = "html")
knit2html("test.Rmd")
---
title: "Reproducible Research: Peer Assessment 1"
author: "Poojan Bhandari"
output:
html_document:
keep_md: true
---
## Loading required library
```{r echo = TRUE}
library(lattice)
```
-----
## Downloading data
```{r echo = TRUE}
url <- "https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2Factivity.zip"
download.file(url,"data.zip")
unzip("data.zip",exdir = "data")
```
-----
## Loading and preprocessing the data
Show any code that is needed to
####1. Load the data (i.e. read.csv())
####2. Process/transform the data (if necessary) into a format suitable for your analysis
```{r echo = TRUE}
filename <- paste("./data",list.files("data"), sep = "/")
activity <- read.csv(filename, header = TRUE, colClasses = c("numeric","character","numeric"), na = "NA")
```
## What is mean total number of steps taken per day?
```
For this part of the assignment, you can ignore the missing values in the dataset.
```
####1. Calculate the total number of steps taken per day
```{r echo = TRUE}
stepsdata <- aggregate(activity$steps, by =list(activity$date), FUN = sum, na.rm =TRUE)
names(stepsdata) <- c("date", "total.steps")
```
####2. If you do not understand the difference between a histogram and a barplot, research the difference between them. Make a histogram of the total number of steps taken each day
```{r echo = TRUE}
histogram(stepsdata$total.steps, xlab = "Total Steps",ylab = "Frequency", main ="total number of steps taken each day", breaks =  20)
```
####3. Calculate and report the mean and median of the total number of steps taken per day
```{r echo = TRUE}
beforestepsmean <- mean(stepsdata$total.steps, na.rm = TRUE)
beforestepsmedian <- median(stepsdata$total.steps, na.rm = TRUE)
beforestepsmean
beforestepsmedian
```
-----
## What is the average daily activity pattern?
####1. Make a time series plot (i.e. type = "l") of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all days (y-axis)
```{r echo = TRUE}
intervaldata <- aggregate(x= list(steps = activity$steps), by =list(interval = activity$interval), FUN = mean, na.rm = TRUE)
xyplot(intervaldata$steps ~ intervaldata$interval, type = "l", xlab ="5-minute interval", ylab = "the average number of steps taken", main = "Average daily activity pattern")
```
####2. Which 5-minute interval, on average across all the days in the dataset, contains the maximum number of steps?
```{r echo = TRUE}
maxintervalsteps <- intervaldata[which.max(intervaldata$steps),]
maxintervalsteps
```
* 5-minute interval with maximum number of steps: `r maxintervalsteps[1,1]`
-----
## Imputing missing values
```
Note that there are a number of days/intervals where there are missing values (coded as NA). The presence of missing days may introduce bias into some calculations or summaries of the data.
```
####1. Calculate and report the total number of missing values in the dataset (i.e. the total number of rows with NAs)
```{r echo = TRUE}
emptydata <- is.na(activity$steps)
table(emptydata)
```
####2. Devise a strategy for filling in all of the missing values in the dataset. The strategy does not need to be sophisticated. For example, you could use the mean/median for that day, or the mean for that 5-minute interval, etc.
```{r echo = TRUE}
fill.data <- function(activity){
for (i in 1:nrow(activity)){
if(is.na(activity[i,1])){
activity[i,"steps"] <- intervaldata[intervaldata$interval == activity[i,"interval"],"steps"]
}
}
return(activity)
}
```
####3. Create a new dataset that is equal to the original dataset but with the missing data filled in.
```{r echo = TRUE}
activityfilled <- fill.data(activity)
```
####4. Make a histogram of the total number of steps taken each day and Calculate and report the mean and median total number of steps taken per day. Do these values differ from the estimates from the first part of the assignment? What is the impact of imputing missing data on the estimates of the total daily number of steps?
```{r echo = TRUE}
stepsdata <- aggregate(activityfilled$steps, by =list(activityfilled$date), function(x){sum(x)})
names(stepsdata) <- c("date","total.steps")
histogram(stepsdata$total.steps, xlab = "Total Steps", ylab = "Frequency", main ="total number of steps taken each day", breaks =  20)
afterstepsmean <- mean(stepsdata$total.steps, na.rm = TRUE)
afterstepsmedian <- median(stepsdata$total.steps, na.rm = TRUE)
afterstepsmean
afterstepsmedian
```
When NA was removed, there was 53 days records for steps in different interval. 8 records were removed and the mean and median was calculated. After replacing 8 records with mean of that interval, mean and median changes.
## Are there differences in activity patterns between weekdays and weekends?
```
For this part the weekdays() function may be of some help here. Use the dataset with the filled-in missing values for this part.
```
####1. Create a new factor variable in the dataset with two levels – “weekday” and “weekend” indicating whether a given date is a weekday or weekend day.
```{r echo = TRUE}
get_day <- function(date){
weekday <- weekdays(as.Date(date))
if(weekday %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday")){
return("weekday")
} else if(weekday %in% c("Saturday", "Sunday")){
return("weekend")
} else {
stop("Not a valid date")
}
}
tempdata <- activityfilled
tempdata$weekday <- sapply(tempdata$date, FUN = get_day)
```
####2. Make a panel plot containing a time series plot (i.e. type = "l") of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all weekday days or weekend days (y-axis). See the README file in the GitHub repository to see an example of what this plot should look like using simulated data.
```{r echo = TRUE}
aggdata <- aggregate(steps ~ interval + weekday, data=tempdata, mean)
with(aggdata,
xyplot(steps ~ interval | weekday, type="l", xlab = "Interval", ylab = "Number of steps", layout = c(1, 2)))
```
knit2html("PA1_template.Rmd")
knit2html("PA1_template.Rmd")
